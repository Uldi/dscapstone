---
title: "Predict Next Word"
author: "David Ulrich"
date: "16.06.2019"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## The Assignment: Predict Next Word
The Datascience Capstone assignment is to predict the next word based on a model to be trained out of a provided text corpus. The idea of the project is based on the wellknown SwiftKey app.

* Text corpus based on about 600MB of US english news, twitter and blog texts
* Corpus split into train(90%), test(5%) & verification(5%) data
* Corpus cleaned by removal of white space, profanity words, punctuation, numbers, urls etc.
* Basic idea is to train a model on n-grams (4-, 3-, 2- and 1-grams)
* Used Quanteda package to clean corpus and build n-grams
* Improved training efficiency by factors using dpylr package


## Model Building Approaches
During the model building phase, I tried several different approaches

* First tests used TM/RWeka for tokenization and n-gram building, showed to be rather slow --> switched to Quanteda
* First version of model used [Stupid Backoff](https://www.aclweb.org/anthology/D07-1090.pdf) approach without any smoothing. Works well for large training corpus.
* Next version used the [Laplace Add-1 smoothing](https://en.wikipedia.org/wiki/Additive_smoothing). But the results didn't improve the prediction rate.
* Tested models with/without stopwords included and models with different minimal counts per stopwords
* Tested all model variants with the same test set of sentence to ensure comparability


## Final Model Approach
For the final model Katz's back-off algorithm with [Good-Touring](https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation) estimation was used (see [Katz's back-off algorithm](https://en.wikipedia.org/wiki/Katz%27s_back-off_model))

<img src="fig/katz.png" alt="Katz formula" width="100%">

where C(x) = number of times x appears in training and wi = *i*th word in the given context

In short Katz's back-Off algorithm backs-off to lower n-1 grams if not found in n grams


## Deployment - Model Tuning
In order to use the model as part of the shiny app, some model tuning (reduction in size) was needed

* Removed all ngrams that end with a stopword as the prediction of a stopword does add much value
* Kept only the ngram/next word combinations with the highest probabilities
* Removed all the 4- and 3-grams which occured only once in the corpus
* Saved ngram tables as RDS files, one file per ngram size (1-4)

--> This resulted in a reduction of the initial model size to less than 100MB

## The Shiny App - Predict Next Word
<div class="columns-2">
<img src="fig/pnw.png" alt="Shinyapp screensho" width="100%">

Main features 

* Loads model on startup
* Uses Katz back-off with Good Touring 
* Allows a natural typing of text
* Provides prediction of the next word on demand
* For prediction just enter 2 spaces or click on the   button *predict*
</div>
https://uldi8400.shinyapps.io/David_DSCapstone/



